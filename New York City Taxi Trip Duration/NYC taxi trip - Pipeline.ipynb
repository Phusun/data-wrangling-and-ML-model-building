{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York City Taxi Trip Duration\n",
    "\n",
    "kaggle link - https://www.kaggle.com/c/nyc-taxi-trip-duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Haversine formula to compute the great-circle distance between two points on a sphere given their longitudes and latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 3956*1609.34 # Radius of earth in meters. Use 3956 for miles. Use 6371 for kilometers.\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class definition for training data transformation and fit and transform methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TrainDataProcessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, filtering=True):\n",
    "        self.filtering = filtering\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if self.filtering:\n",
    "            distance = X.apply(lambda df:haversine(df['pickup_longitude'], df['pickup_latitude'],\n",
    "                                                   df['dropoff_longitude'],df['dropoff_latitude']), axis=1)\n",
    "            highest_speed = distance/(X['trip_duration'])\n",
    "            return X.drop(X[(X['trip_duration']<30)|(X['trip_duration']>10000)|(highest_speed>50)|(highest_speed<0.5)].index)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data split into training and validation so that an estimate of accuracy on test data can be obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "train_data, val_data = train_test_split(train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering and transformation: predictors and response are separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering = True\n",
    "\n",
    "train_data_filt = TrainDataProcessor(filtering).fit_transform(train_data)\n",
    "\n",
    "X_train = train_data_filt.drop(columns=['trip_duration'])\n",
    "y_train = train_data_filt['trip_duration']\n",
    "X_val = val_data.drop(columns=['trip_duration'])\n",
    "y_val = val_data['trip_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class definition for adding features and fit and transform methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Columns to add: pickup_hour, pickup_dayofweek, distance \n",
    "\n",
    "class FeaturesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X['pickup_hour'] = pd.to_datetime(X['pickup_datetime']).dt.hour\n",
    "        X['pickup_dayofweek'] = pd.to_datetime(X['pickup_datetime']).dt.dayofweek\n",
    "        X['pickup_month'] = pd.to_datetime(X['pickup_datetime']).dt.month\n",
    "        X['distance'] = X.apply(lambda df:haversine(df['pickup_longitude'], df['pickup_latitude'],\n",
    "                                                    df['dropoff_longitude'],df['dropoff_latitude']), axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class definition for attributes/predictors selection and fit and transform methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class definition for category encoding and fit and transform methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_encode):\n",
    "        self.cols_to_encode = cols_to_encode\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return pd.get_dummies(X, columns=self.cols_to_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full pipeline construction for training and validation data transformation. Pipeline performs separately on numerical and categorical features in the data and then joins them together. Following operations are done:\n",
    "- feature engineering\n",
    "- atrributes selection\n",
    "- scaling\n",
    "- category encoding\n",
    "- features union\n",
    "\n",
    "#### Training and validation data are ready for ML models after this code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "cols_to_encode = ['vendor_id','store_and_fwd_flag','pickup_dayofweek', 'pickup_month']\n",
    "cols_to_scale = ['pickup_hour','distance','passenger_count']\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('features_adder', FeaturesAdder()),\n",
    "    ('num_cat_FU', FeatureUnion([\n",
    "        ('num_pipe', Pipeline([\n",
    "            ('selector', DataFrameSelector(cols_to_scale)),\n",
    "            ('std_scaler', StandardScaler())\n",
    "        ])),\n",
    "        ('cat_pipe', Pipeline([\n",
    "            ('selector', DataFrameSelector(cols_to_encode)),\n",
    "            ('one_hot', CatEncoder(cols_to_encode))\n",
    "        ]))\n",
    "    ])),    \n",
    "])\n",
    "\n",
    "train_prepared = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "val_prepared = full_pipeline.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM is applied on the training data and Root Mean Squared Logarithmic Error (RMSLE) for validation data is computed since RMSLE is the evaluation metric for this dataset, as stated on kaggle page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_reg = lgb.LGBMRegressor(n_estimators=200)\n",
    "lgb_reg.fit(train_prepared, y_train)\n",
    "y_pred = lgb_reg.predict(val_prepared)\n",
    "val_error = np.sqrt(mean_squared_log_error(y_val, y_pred))\n",
    "print('Validation RMSLE with lgbm:', val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost and Gradient boosting can also be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor(max_depth=4, n_estimators=250)\n",
    "xgb_reg.fit(train_prepared, y_train)\n",
    "y_pred = xgb_reg.predict(val_prepared)\n",
    "val_error = np.sqrt(mean_squared_log_error(y_val, y_pred))\n",
    "print('Validation RMSLE with xgboost:', val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=5, n_estimators=300, learning_rate=0.1)\n",
    "gbrt.fit(train_prepared, y_train)\n",
    "y_pred = gbrt.predict(val_prepared)\n",
    "val_error = np.sqrt(mean_squared_log_error(y_val, y_pred))\n",
    "print('Validation RMSLE with GradientBoosting:', val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the test data through full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepared = full_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the predictions and append it to test data before writing it into a _csv_ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.around(xgb_reg.predict(test_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['id'] = test['id']\n",
    "result['trip_duration'] = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
